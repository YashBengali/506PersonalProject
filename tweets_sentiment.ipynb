{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1300 entries, 0 to 1299\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          1300 non-null   int64 \n",
      " 1   created_at  1300 non-null   object\n",
      " 2   text        1300 non-null   object\n",
      " 3   Score       1300 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 50.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_stock_tweets_labled():\n",
    "    df = pd.read_csv('tweets_labelled.csv', sep=';')\n",
    "    df.dropna(inplace=True)\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x:1 if x == 'positive' else 0)\n",
    "    df = df.rename(columns={\"sentiment\": \"Score\"})\n",
    "    return df\n",
    "\n",
    "df = read_stock_tweets_labled()\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed\n",
      "here\n",
      "this done\n",
      "shape\n",
      "(1300, 60)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1300 entries, 0 to 1299\n",
      "Data columns (total 64 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          1300 non-null   int64  \n",
      " 1   created_at  1300 non-null   object \n",
      " 2   text        1300 non-null   object \n",
      " 3   Score       1300 non-null   int64  \n",
      " 4   svd0        1300 non-null   float64\n",
      " 5   svd1        1300 non-null   float64\n",
      " 6   svd2        1300 non-null   float64\n",
      " 7   svd3        1300 non-null   float64\n",
      " 8   svd4        1300 non-null   float64\n",
      " 9   svd5        1300 non-null   float64\n",
      " 10  svd6        1300 non-null   float64\n",
      " 11  svd7        1300 non-null   float64\n",
      " 12  svd8        1300 non-null   float64\n",
      " 13  svd9        1300 non-null   float64\n",
      " 14  svd10       1300 non-null   float64\n",
      " 15  svd11       1300 non-null   float64\n",
      " 16  svd12       1300 non-null   float64\n",
      " 17  svd13       1300 non-null   float64\n",
      " 18  svd14       1300 non-null   float64\n",
      " 19  svd15       1300 non-null   float64\n",
      " 20  svd16       1300 non-null   float64\n",
      " 21  svd17       1300 non-null   float64\n",
      " 22  svd18       1300 non-null   float64\n",
      " 23  svd19       1300 non-null   float64\n",
      " 24  svd20       1300 non-null   float64\n",
      " 25  svd21       1300 non-null   float64\n",
      " 26  svd22       1300 non-null   float64\n",
      " 27  svd23       1300 non-null   float64\n",
      " 28  svd24       1300 non-null   float64\n",
      " 29  svd25       1300 non-null   float64\n",
      " 30  svd26       1300 non-null   float64\n",
      " 31  svd27       1300 non-null   float64\n",
      " 32  svd28       1300 non-null   float64\n",
      " 33  svd29       1300 non-null   float64\n",
      " 34  svd30       1300 non-null   float64\n",
      " 35  svd31       1300 non-null   float64\n",
      " 36  svd32       1300 non-null   float64\n",
      " 37  svd33       1300 non-null   float64\n",
      " 38  svd34       1300 non-null   float64\n",
      " 39  svd35       1300 non-null   float64\n",
      " 40  svd36       1300 non-null   float64\n",
      " 41  svd37       1300 non-null   float64\n",
      " 42  svd38       1300 non-null   float64\n",
      " 43  svd39       1300 non-null   float64\n",
      " 44  svd40       1300 non-null   float64\n",
      " 45  svd41       1300 non-null   float64\n",
      " 46  svd42       1300 non-null   float64\n",
      " 47  svd43       1300 non-null   float64\n",
      " 48  svd44       1300 non-null   float64\n",
      " 49  svd45       1300 non-null   float64\n",
      " 50  svd46       1300 non-null   float64\n",
      " 51  svd47       1300 non-null   float64\n",
      " 52  svd48       1300 non-null   float64\n",
      " 53  svd49       1300 non-null   float64\n",
      " 54  svd50       1300 non-null   float64\n",
      " 55  svd51       1300 non-null   float64\n",
      " 56  svd52       1300 non-null   float64\n",
      " 57  svd53       1300 non-null   float64\n",
      " 58  svd54       1300 non-null   float64\n",
      " 59  svd55       1300 non-null   float64\n",
      " 60  svd56       1300 non-null   float64\n",
      " 61  svd57       1300 non-null   float64\n",
      " 62  svd58       1300 non-null   float64\n",
      " 63  svd59       1300 non-null   float64\n",
      "dtypes: float64(60), int64(2), object(2)\n",
      "memory usage: 660.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def process(df):\n",
    "    #trying tfidf \n",
    "    stemmed_data = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in  df['text']]\n",
    "    print('stemmed')\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.8)\n",
    "    dtm = vectorizer.fit_transform(stemmed_data)\n",
    "    num_features = 60\n",
    "    svd = TruncatedSVD(num_features)\n",
    "    print('here')\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    print('this done')\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    dtm = lsa.fit_transform(dtm)\n",
    "    print('shape')\n",
    "    print(np.shape(dtm))\n",
    "    dtm = np.transpose(dtm)\n",
    "    columns = ['svd' + str(i) for i in range(num_features)]\n",
    "    # svd = pd.DataFrame(dtm, columns=columns)\n",
    "    # svd.fillna(0)\n",
    "    # labels= ['SV'+str(i) for i num_features]\n",
    "    for i in range(num_features):\n",
    "        name = 'svd' + str(i)\n",
    "        # print(np.shape(dtm[:][i]))\n",
    "        df[name] = dtm[i]\n",
    "        df[name] = df[name].fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "process(df)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing the infos\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 975 entries, 1268 to 684\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   svd0    975 non-null    float64\n",
      " 1   svd1    975 non-null    float64\n",
      " 2   svd2    975 non-null    float64\n",
      " 3   svd3    975 non-null    float64\n",
      " 4   svd4    975 non-null    float64\n",
      " 5   svd5    975 non-null    float64\n",
      " 6   svd6    975 non-null    float64\n",
      " 7   svd7    975 non-null    float64\n",
      " 8   svd8    975 non-null    float64\n",
      " 9   svd9    975 non-null    float64\n",
      " 10  svd10   975 non-null    float64\n",
      " 11  svd11   975 non-null    float64\n",
      " 12  svd12   975 non-null    float64\n",
      " 13  svd13   975 non-null    float64\n",
      " 14  svd14   975 non-null    float64\n",
      " 15  svd15   975 non-null    float64\n",
      " 16  svd16   975 non-null    float64\n",
      " 17  svd17   975 non-null    float64\n",
      " 18  svd18   975 non-null    float64\n",
      " 19  svd19   975 non-null    float64\n",
      " 20  svd20   975 non-null    float64\n",
      " 21  svd21   975 non-null    float64\n",
      " 22  svd22   975 non-null    float64\n",
      " 23  svd23   975 non-null    float64\n",
      " 24  svd24   975 non-null    float64\n",
      " 25  svd25   975 non-null    float64\n",
      " 26  svd26   975 non-null    float64\n",
      " 27  svd27   975 non-null    float64\n",
      " 28  svd28   975 non-null    float64\n",
      " 29  svd29   975 non-null    float64\n",
      " 30  svd30   975 non-null    float64\n",
      " 31  svd31   975 non-null    float64\n",
      " 32  svd32   975 non-null    float64\n",
      " 33  svd33   975 non-null    float64\n",
      " 34  svd34   975 non-null    float64\n",
      " 35  svd35   975 non-null    float64\n",
      " 36  svd36   975 non-null    float64\n",
      " 37  svd37   975 non-null    float64\n",
      " 38  svd38   975 non-null    float64\n",
      " 39  svd39   975 non-null    float64\n",
      " 40  svd40   975 non-null    float64\n",
      " 41  svd41   975 non-null    float64\n",
      " 42  svd42   975 non-null    float64\n",
      " 43  svd43   975 non-null    float64\n",
      " 44  svd44   975 non-null    float64\n",
      " 45  svd45   975 non-null    float64\n",
      " 46  svd46   975 non-null    float64\n",
      " 47  svd47   975 non-null    float64\n",
      " 48  svd48   975 non-null    float64\n",
      " 49  svd49   975 non-null    float64\n",
      " 50  svd50   975 non-null    float64\n",
      " 51  svd51   975 non-null    float64\n",
      " 52  svd52   975 non-null    float64\n",
      " 53  svd53   975 non-null    float64\n",
      " 54  svd54   975 non-null    float64\n",
      " 55  svd55   975 non-null    float64\n",
      " 56  svd56   975 non-null    float64\n",
      " 57  svd57   975 non-null    float64\n",
      " 58  svd58   975 non-null    float64\n",
      " 59  svd59   975 non-null    float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 464.6 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 325 entries, 579 to 82\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   svd0    325 non-null    float64\n",
      " 1   svd1    325 non-null    float64\n",
      " 2   svd2    325 non-null    float64\n",
      " 3   svd3    325 non-null    float64\n",
      " 4   svd4    325 non-null    float64\n",
      " 5   svd5    325 non-null    float64\n",
      " 6   svd6    325 non-null    float64\n",
      " 7   svd7    325 non-null    float64\n",
      " 8   svd8    325 non-null    float64\n",
      " 9   svd9    325 non-null    float64\n",
      " 10  svd10   325 non-null    float64\n",
      " 11  svd11   325 non-null    float64\n",
      " 12  svd12   325 non-null    float64\n",
      " 13  svd13   325 non-null    float64\n",
      " 14  svd14   325 non-null    float64\n",
      " 15  svd15   325 non-null    float64\n",
      " 16  svd16   325 non-null    float64\n",
      " 17  svd17   325 non-null    float64\n",
      " 18  svd18   325 non-null    float64\n",
      " 19  svd19   325 non-null    float64\n",
      " 20  svd20   325 non-null    float64\n",
      " 21  svd21   325 non-null    float64\n",
      " 22  svd22   325 non-null    float64\n",
      " 23  svd23   325 non-null    float64\n",
      " 24  svd24   325 non-null    float64\n",
      " 25  svd25   325 non-null    float64\n",
      " 26  svd26   325 non-null    float64\n",
      " 27  svd27   325 non-null    float64\n",
      " 28  svd28   325 non-null    float64\n",
      " 29  svd29   325 non-null    float64\n",
      " 30  svd30   325 non-null    float64\n",
      " 31  svd31   325 non-null    float64\n",
      " 32  svd32   325 non-null    float64\n",
      " 33  svd33   325 non-null    float64\n",
      " 34  svd34   325 non-null    float64\n",
      " 35  svd35   325 non-null    float64\n",
      " 36  svd36   325 non-null    float64\n",
      " 37  svd37   325 non-null    float64\n",
      " 38  svd38   325 non-null    float64\n",
      " 39  svd39   325 non-null    float64\n",
      " 40  svd40   325 non-null    float64\n",
      " 41  svd41   325 non-null    float64\n",
      " 42  svd42   325 non-null    float64\n",
      " 43  svd43   325 non-null    float64\n",
      " 44  svd44   325 non-null    float64\n",
      " 45  svd45   325 non-null    float64\n",
      " 46  svd46   325 non-null    float64\n",
      " 47  svd47   325 non-null    float64\n",
      " 48  svd48   325 non-null    float64\n",
      " 49  svd49   325 non-null    float64\n",
      " 50  svd50   325 non-null    float64\n",
      " 51  svd51   325 non-null    float64\n",
      " 52  svd52   325 non-null    float64\n",
      " 53  svd53   325 non-null    float64\n",
      " 54  svd54   325 non-null    float64\n",
      " 55  svd55   325 non-null    float64\n",
      " 56  svd56   325 non-null    float64\n",
      " 57  svd57   325 non-null    float64\n",
      " 58  svd58   325 non-null    float64\n",
      " 59  svd59   325 non-null    float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 154.9 KB\n",
      "None\n",
      "(975, 60) (975,)\n",
      "(325, 60) (325,)\n",
      "got the predictions\n",
      "325\n",
      "325 325\n",
      "RMSE on testing set =  0.253420833544014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn import utils\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load files into DataFrames\n",
    "\n",
    "\n",
    "def get_model(df):\n",
    "\n",
    "    X_train = df\n",
    "\n",
    "    # X_train = utils.shuffle(X_train, random_state=1)\n",
    "\n",
    "    # Split training set into training and testing set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X_train.drop(['Score'], axis=1),\n",
    "            X_train['Score'],\n",
    "            test_size=1/4.0,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "    X_train_processed = X_train.drop(columns=['id', 'created_at', 'text'])\n",
    "    X_test_processed = X_test.drop(columns=['id', 'created_at', 'text'])\n",
    "    # X_submission_processed = X_submission.drop(columns=['Id', 'Time', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "    # print(Y_train.head)\n",
    "    # Y_train = Y_train.fillna(4) #shouldn't need to do this\n",
    "    # print(Y_train.isnull().values.any())\n",
    "\n",
    "    # print(Y_test.head)\n",
    "    # print(Y_test.isnull().values.any())\n",
    "    # Y_test = Y_test.fillna(4) #shouldn't need to do this\n",
    "\n",
    "    print('printing the infos')\n",
    "    print(X_train_processed.info())\n",
    "    print(X_test_processed.info())\n",
    "    # print(X_train_processed.isnull().values.any())\n",
    "\n",
    "\n",
    "    print((X_train_processed.shape), Y_train.shape)\n",
    "    print((X_test_processed.shape), Y_test.shape)\n",
    "\n",
    "\n",
    "    # print(X_train[X_train.columns[0]].head(10))\n",
    "    # subX_train = (X_train['good_cnt']) / (X_train['good_cnt'] + X_train['bad_cnt'] + X_train['okay_cnt'])\n",
    "    # # subX_test = X_test[X_test['Score']]\n",
    "    # plt.scatter(subX_train, Y_train, c=\"slategray\", alpha=0.4, linewidths=0.1)\n",
    "    # # plt.scatter(subX_test, y_CA_H_test, c=\"seagreen\", alpha=0.2, linewidths=0.3)\n",
    "    # plt.xlabel('ProductScore')\n",
    "    # plt.ylabel('Actual Score')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # Predict the score using the model\n",
    "\n",
    "    # print(X_train['good_cntSummary'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model = sm.OLS(Y_train, X_train_processed)\n",
    "    # results = model.fit()\n",
    "    # print(results.summary())\n",
    "    # Y_test_predictions = model.predict(np.transpose(X_test_processed))\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train_processed, Y_train)\n",
    "    Y_test_predictions = model.predict(X_test_processed)\n",
    "\n",
    "\n",
    "    # print(Y_test_predictions.isnull().values.any())\n",
    "    # print(Y_test.isnull().values.any())\n",
    "    print('got the predictions')\n",
    "    print(len(Y_test_predictions))\n",
    "    # Y_test_predictions = Y_test_predictions.round()\n",
    "    # Y_test_predictions = [min(5, max(1, x)) for x in Y_test_predictions] \n",
    "\n",
    "    print(str(len(Y_test)) + ' ' + str(len(Y_test_predictions)))\n",
    "\n",
    "    print(\"RMSE on testing set = \", mean_squared_error(Y_test, Y_test_predictions))\n",
    "\n",
    "    # prediction['Score'] = ret\n",
    "\n",
    "    # submission = prediction[['Id', 'Score']]\n",
    "    # print(Y_test_predictions.head())\n",
    "    # Y_test_predictions.to_csv(\"./data/y_test_submission.csv\", index=False)\n",
    "\n",
    "    # Plot a confusion matrix\n",
    "    # cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "    # sns.heatmap(cm, annot=True)\n",
    "    # plt.title('Confusion matrix of the classifier')\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.ylabel('True')\n",
    "    # plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed\n",
      "here\n",
      "this done\n",
      "shape\n",
      "(3717964, 60)\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"Tweet.csv\")\n",
    "\n",
    "tweets = tweets.rename(columns={'body': 'text'})\n",
    "tweets.drop(columns=['writer', 'comment_num','retweet_num','like_num'])\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(lambda x : '' if not (type(x) == type('a')) else x)\n",
    "\n",
    "tweets = process(tweets)\n",
    "\n",
    "tweets.to_csv(\"tweets_processed.csv\", index=False)\n",
    "\n",
    "tweets.info()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3530306 entries, 0 to 3530305\n",
      "Data columns (total 68 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   tweet_id     int64  \n",
      " 1   writer       object \n",
      " 2   post_date    int64  \n",
      " 3   text         object \n",
      " 4   comment_num  int64  \n",
      " 5   retweet_num  int64  \n",
      " 6   like_num     int64  \n",
      " 7   svd0         float64\n",
      " 8   svd1         float64\n",
      " 9   svd2         float64\n",
      " 10  svd3         float64\n",
      " 11  svd4         float64\n",
      " 12  svd5         float64\n",
      " 13  svd6         float64\n",
      " 14  svd7         float64\n",
      " 15  svd8         float64\n",
      " 16  svd9         float64\n",
      " 17  svd10        float64\n",
      " 18  svd11        float64\n",
      " 19  svd12        float64\n",
      " 20  svd13        float64\n",
      " 21  svd14        float64\n",
      " 22  svd15        float64\n",
      " 23  svd16        float64\n",
      " 24  svd17        float64\n",
      " 25  svd18        float64\n",
      " 26  svd19        float64\n",
      " 27  svd20        float64\n",
      " 28  svd21        float64\n",
      " 29  svd22        float64\n",
      " 30  svd23        float64\n",
      " 31  svd24        float64\n",
      " 32  svd25        float64\n",
      " 33  svd26        float64\n",
      " 34  svd27        float64\n",
      " 35  svd28        float64\n",
      " 36  svd29        float64\n",
      " 37  svd30        float64\n",
      " 38  svd31        float64\n",
      " 39  svd32        float64\n",
      " 40  svd33        float64\n",
      " 41  svd34        float64\n",
      " 42  svd35        float64\n",
      " 43  svd36        float64\n",
      " 44  svd37        float64\n",
      " 45  svd38        float64\n",
      " 46  svd39        float64\n",
      " 47  svd40        float64\n",
      " 48  svd41        float64\n",
      " 49  svd42        float64\n",
      " 50  svd43        float64\n",
      " 51  svd44        float64\n",
      " 52  svd45        float64\n",
      " 53  svd46        float64\n",
      " 54  svd47        float64\n",
      " 55  svd48        float64\n",
      " 56  svd49        float64\n",
      " 57  svd50        float64\n",
      " 58  svd51        float64\n",
      " 59  svd52        float64\n",
      " 60  svd53        float64\n",
      " 61  svd54        float64\n",
      " 62  svd55        float64\n",
      " 63  svd56        float64\n",
      " 64  svd57        float64\n",
      " 65  svd58        float64\n",
      " 66  svd59        float64\n",
      " 67  date         object \n",
      "dtypes: float64(60), int64(5), object(3)\n",
      "memory usage: 1.8+ GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3530306 entries, 0 to 3530305\n",
      "Data columns (total 70 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   tweet_id     int64  \n",
      " 1   writer       object \n",
      " 2   post_date    int64  \n",
      " 3   text         object \n",
      " 4   comment_num  int64  \n",
      " 5   retweet_num  int64  \n",
      " 6   like_num     int64  \n",
      " 7   svd0         float64\n",
      " 8   svd1         float64\n",
      " 9   svd2         float64\n",
      " 10  svd3         float64\n",
      " 11  svd4         float64\n",
      " 12  svd5         float64\n",
      " 13  svd6         float64\n",
      " 14  svd7         float64\n",
      " 15  svd8         float64\n",
      " 16  svd9         float64\n",
      " 17  svd10        float64\n",
      " 18  svd11        float64\n",
      " 19  svd12        float64\n",
      " 20  svd13        float64\n",
      " 21  svd14        float64\n",
      " 22  svd15        float64\n",
      " 23  svd16        float64\n",
      " 24  svd17        float64\n",
      " 25  svd18        float64\n",
      " 26  svd19        float64\n",
      " 27  svd20        float64\n",
      " 28  svd21        float64\n",
      " 29  svd22        float64\n",
      " 30  svd23        float64\n",
      " 31  svd24        float64\n",
      " 32  svd25        float64\n",
      " 33  svd26        float64\n",
      " 34  svd27        float64\n",
      " 35  svd28        float64\n",
      " 36  svd29        float64\n",
      " 37  svd30        float64\n",
      " 38  svd31        float64\n",
      " 39  svd32        float64\n",
      " 40  svd33        float64\n",
      " 41  svd34        float64\n",
      " 42  svd35        float64\n",
      " 43  svd36        float64\n",
      " 44  svd37        float64\n",
      " 45  svd38        float64\n",
      " 46  svd39        float64\n",
      " 47  svd40        float64\n",
      " 48  svd41        float64\n",
      " 49  svd42        float64\n",
      " 50  svd43        float64\n",
      " 51  svd44        float64\n",
      " 52  svd45        float64\n",
      " 53  svd46        float64\n",
      " 54  svd47        float64\n",
      " 55  svd48        float64\n",
      " 56  svd49        float64\n",
      " 57  svd50        float64\n",
      " 58  svd51        float64\n",
      " 59  svd52        float64\n",
      " 60  svd53        float64\n",
      " 61  svd54        float64\n",
      " 62  svd55        float64\n",
      " 63  svd56        float64\n",
      " 64  svd57        float64\n",
      " 65  svd58        float64\n",
      " 66  svd59        float64\n",
      " 67  date         object \n",
      " 68  DATE         object \n",
      " 69  sent         float32\n",
      "dtypes: float32(1), float64(60), int64(5), object(4)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "tweets = pd.read_csv('tweets_processed.csv')\n",
    "\n",
    "tweets['date'] = tweets['post_date'].apply(lambda x : str(datetime.fromtimestamp(x)))\n",
    "\n",
    "print(tweets.info())\n",
    "\n",
    "\n",
    "date = tweets.date.str.split(expand=True)\n",
    "tweets[\"DATE\"] = date[0]\n",
    "tweets.sort_values(by=['DATE'])\n",
    "tweets = tweets[tweets.DATE != 'date']\n",
    "tweets = tweets[tweets.DATE != '=======']\n",
    "\n",
    "\n",
    "tweets_dropped = tweets.drop(columns=['date', 'tweet_id', 'writer', 'post_date', 'comment_num', 'retweet_num', 'like_num', 'text', 'DATE'], inplace=False)\n",
    "#     X_test_processed = X_test.drop(columns=['id', 'created_at', 'text'])\n",
    "predictions = model.predict(tweets_dropped)\n",
    "\n",
    "tweets['sent'] = predictions\n",
    "\n",
    "tweets.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Mean_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27</td>\n",
       "      <td>0.146988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>0.467021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>0.236669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>0.790913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>0.562061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2019-09-15</td>\n",
       "      <td>0.237849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>0.450124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>0.509969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>0.478774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>0.578426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Mean_Sentiment\n",
       "0    2015-01-27        0.146988\n",
       "1    2015-03-09        0.467021\n",
       "2    2015-04-28        0.236669\n",
       "3    2015-06-24        0.790913\n",
       "4    2016-01-26        0.562061\n",
       "..          ...             ...\n",
       "211  2019-09-15        0.237849\n",
       "212  2019-09-16        0.450124\n",
       "213  2019-09-17        0.509969\n",
       "214  2019-09-18        0.478774\n",
       "215  2019-09-20        0.578426\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data = pd.read_csv(\"AAPL.csv\")\n",
    "\n",
    "# limit tweets to tweets with over 10 likes\n",
    "\n",
    "tweets = tweets[tweets.apply(lambda x : int(x['like_num']) > 500 and int(x['comment_num']) > 5, axis=1)]\n",
    "\n",
    "\n",
    "grouped_df = tweets.groupby(by = [\"DATE\"])\n",
    "sentiment = []\n",
    "date=[]\n",
    "for name,group in grouped_df:\n",
    "    date.append(name)\n",
    "    sentiment.append(group.sent.mean())\n",
    "stock_df = pd.DataFrame({\"Date\": date, \"Mean_Sentiment\": sentiment})\n",
    "\n",
    "# stock_df['Mean_Sentiment'] = stock_df['Mean_Sentiment'].apply(lambda x  : print(x) if x < 0 else x)\n",
    "stock_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  increased  change_value  Mean_Sentiment\n",
      "0    2014-12-31          0     -0.610001        0.412905\n",
      "1    2015-01-05          0     -0.510000        1.224695\n",
      "2    2015-01-08          1      0.664999        0.594274\n",
      "3    2015-01-09          0     -0.164999        0.606336\n",
      "4    2015-01-14          1      0.190001        0.587961\n",
      "..          ...        ...           ...             ...\n",
      "985  2019-09-16          1      0.542499        0.470423\n",
      "986  2019-09-17          1      0.184997        0.465139\n",
      "987  2019-09-18          1      0.427502        0.439712\n",
      "988  2019-09-19          0     -0.262497        0.345741\n",
      "989  2019-09-20          0     -0.912502        0.418222\n",
      "\n",
      "[990 rows x 4 columns]\n",
      "                increased  change_value  Mean_Sentiment\n",
      "increased        1.000000      0.718894        0.028701\n",
      "change_value     0.718894      1.000000        0.053212\n",
      "Mean_Sentiment   0.028701      0.053212        1.000000\n"
     ]
    }
   ],
   "source": [
    "stock_data[\"increased\"] = np.where(stock_data[\"Close\"]>stock_data[\"Open\"], 1, 0)\n",
    "stock_data[\"change_value\"] = stock_data[\"Close\"]-stock_data[\"Open\"]\n",
    "# stock_data['Date'] = stock_data['date']\n",
    "\n",
    "# print(stock_data)\n",
    "# print(stock_df)\n",
    "\n",
    "x = pd.merge(stock_data, stock_df, on=\"Date\")\n",
    "# print(x)\n",
    "# x['Mean_Sentiment'] = x.Mean_Sentiment.shift(-1, fill_value=0)\n",
    "# x = x[x.apply(lambda x : int(x['Date'][:4]) > 2017, axis=1)]\n",
    "x = x.drop(columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "print(x)\n",
    "print(x.corr())\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# print(x[\"change_value\"].corr(x[\"Mean_Sentiment\"]))\n",
    "# print(x[\"increased\"].corr(x[\"Mean_Sentiment\"]))\n",
    "# print(pearsonr(x[\"change_value\"],x[\"Mean_Sentiment\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
